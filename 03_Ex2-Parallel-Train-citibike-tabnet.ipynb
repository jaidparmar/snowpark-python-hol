{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineering Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Feature Engineering, Training and Inference\n",
    "\n",
    "Exercise: The original code was built from feature engineering, training, inference and evaluation on a single station.  The ML Engineer initially took this code and wrapped it in various loops to operate over all stations.  This is REALLY SLOW.  Instead we would like to vectorize as much of the code as possible, push-down all operations into Snowflake and leverage the parallel nature of Snowpark User-defined Functions.\n",
    "\n",
    "Snowpark user-defined functions (UDFs) are generally a good candidte for the so-called \"Embarrassingly parallel\" workloads.   The use of Snowpark Python UDFs for training (in this code) is a stretch of the original intention of UDFs.  However, this code is meant to give an idea of the art of the possible.\n",
    "\n",
    "**Note**: At the current time in Snowpark Python Private Preview the row-based parallelization is an additional feature flag which is not enabled on all accounts with Snowpark python.  Much of the work can be parallelized without this feature but the final training is currently limited in parallelization.\n",
    "\n",
    "**Note**: At the current time in Snowpark Python Private Preview the Snowpark Python UDFs are limited to scalar functions (one row in, one value out).  Much of the complexity around parallelizing UDFs comes from the current lack of user-defined table functions and this code will get MUCH easier in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.  Feature engineering, train, predict functions from data scientist.  \n",
    "Output: Prediction models available to business users in SQL. Evaluation reports for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Materialize the holidays and weather features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "from citibike_ml.mlops_pipeline import materialize_holiday_table, materialize_weather_table\n",
    "\n",
    "trips_table_name = state_dict['trips_table_name']\n",
    "#holiday_table_name = state_dict['holiday_table_name']\n",
    "#weather_table_name = state_dict['weather_table_name']\n",
    "#model_stage_name = state_dict['model_stage_name']\n",
    "\n",
    "holiday_table_name = materialize_holiday_table(session=session,\n",
    "                                               holiday_table_name=state_dict['holiday_table_name'])\n",
    "precip_table_name = materialize_precip_table(session=session, \n",
    "                                             weather_table_name=state_dict['weather_table_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Create a vectorized feature generation\n",
    "Previously the data scientist picked one station for training and predictions.  We want to generate features for all stations in parallel.  We can leverage the power of the Snowflake SQL execution engine for this but Snowpark allows us to write it in python.  \n",
    "\n",
    "Snowflake [window functions](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.html#snowflake.snowpark.Window) are a powerful tool for vectorizing work.  Our initial feature engineering code from the data scientist used window functions to calculate the lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp \n",
    "trips_df = session.table(trips_table_name)\n",
    "holiday_df = session.table(holiday_table_name)\n",
    "precip_df = session.table(precip_table_name)\n",
    "station_id = '519'\n",
    "\n",
    "date_window = snp.Window.orderBy('DATE')\n",
    "\n",
    "#Previously start with a filter on station_id\n",
    "\n",
    "feature_df = trips_df.filter(F.col('START_STATION_ID') == station_id)\\\n",
    "                     .select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                             F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                     .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                        .count()\\\n",
    "                     .withColumn('LAG_1', F.lag(F.col('COUNT'), offset=1, default_value=None).over(date_window))\\\n",
    "                     .withColumn('LAG_7', F.lag(F.col('COUNT'), offset=7, default_value=None).over(date_window))\\\n",
    "                        .na.drop()\\\n",
    "                     .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                     .join(precip_df, 'DATE', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.show(5), feature_df.select('STATION_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a multi-level window function to allow us to partition by station_id and group by the date within that window.  \n",
    "  \n",
    "Notice there is no `filter()` initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_date_window = snp.Window.partitionBy(F.col('STATION_ID')).orderBy(F.col('DATE').asc())\n",
    "\n",
    "feature_df = trips_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                             F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                     .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                        .count()\\\n",
    "                     .withColumn('LAG_1', F.lag(F.col('COUNT'), offset=1, default_value=None).over(sid_date_window))\\\n",
    "                     .withColumn('LAG_7', F.lag(F.col('COUNT'), offset=7, default_value=None).over(sid_date_window))\\\n",
    "                        .na.drop()\\\n",
    "                     .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                     .join(precip_df, 'DATE', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.show(5), feature_df.select('STATION_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature dataframe now has feature sets for 1061 of the original 1081 stations.  Twenty stations have less than 7 total trips so these end up being dropped because our `lag()` functions are not imputing missing values but rather dropping them.  \n",
    "  \n",
    "  \n",
    "There is one more step.  Our upstream feature training will do a 365 day split using the first year for training and the second year for validation.  This is important because of the annual seasonality that our model needs to capture.  \n",
    "So we need to generate features only for stations that have at least 2 years worth of data.  Again, we can do this with a second window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_date_window = snp.Window.partitionBy(F.col('STATION_ID')).orderBy(F.col('DATE').asc())\n",
    "sid_window = snp.Window.partitionBy(F.col('STATION_ID'))\n",
    "\n",
    "\n",
    "feature_df = trips_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                             F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                     .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                        .count()\\\n",
    "                     .withColumn('LAG_1', F.lag(F.col('COUNT'), offset=1, default_value=None).over(sid_date_window))\\\n",
    "                     .withColumn('LAG_7', F.lag(F.col('COUNT'), offset=7, default_value=None).over(sid_date_window))\\\n",
    "                        .na.drop()\\\n",
    "                     .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                     .join(precip_df, 'DATE', 'inner')\\\n",
    "                     .withColumn('DAY_COUNT', F.count(F.col('DATE')).over(sid_window))\\\n",
    "                        .filter(F.col('DAY_COUNT') >= 365*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature set should not include any stations with less than 737 days (365*2+7) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.select(F.min('DAY_COUNT')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how many stations have at least two years of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.select('STATION_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Vectorize the training and inference\n",
    "\n",
    "Because we currently only have scalar functions for Snowpark Python UDFs we must aggregate the features for each station to a single cell.   This will be much easier in the future with vectorized input and user-defined table functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.drop('DAY_COUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_column_list = feature_df.columns\n",
    "feature_column_list.remove('\\\"STATION_ID\\\"')\n",
    "feature_column_list = [f.replace('\\\"', \"\") for f in feature_column_list]\n",
    "feature_column_array = F.array_construct(*[F.lit(x) for x in feature_column_list])\n",
    "\n",
    "feature_df_stuffed = feature_df.groupBy(F.col('STATION_ID'))\\\n",
    "                               .agg(F.array_agg(F.array_construct(*feature_column_list)).alias('INPUT_DATA'))\\\n",
    "                               .withColumn('INPUT_COLUMN_LIST', feature_column_array)\\\n",
    "                               .withColumn('TARGET_COLUMN', F.lit('COUNT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_stuffed.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check to make sure the aggregate happened at the right level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Update the Training/Prediction Code to use UDF parallelization\n",
    "Now that we can generate the features in parallel we can also use the Snowflake UDF structure to train all of our stations in parallel.  The handler will run 8x per node of the warehouse so to train on 1061 stations we will need a larger warehouse.  \n",
    "  \n",
    "First we need to update our UDF handler with the 2-year logic as well in case someone accidentally calls it with data that wasn't filter for 2 year minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /station_train_predict.py\n",
    "\n",
    "def station_train_predict_func(input_data: list, \n",
    "                               input_columns: list, \n",
    "                               target_column: str,\n",
    "                               max_epochs: int) -> str:\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(input_data, columns = input_columns)\n",
    "    \n",
    "    #Due to annual seasonality we need at least one year of data for training \n",
    "    #and a second year of data for validation\n",
    "    if len(df) < 365*2:\n",
    "        df['PRED'] = 'NULL'\n",
    "    else:\n",
    "        feature_columns = input_columns.copy()\n",
    "        feature_columns.remove('DATE')\n",
    "        feature_columns.remove(target_column)\n",
    "\n",
    "        from torch import tensor\n",
    "        from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "        model = TabNetRegressor()\n",
    "\n",
    "        #cutpoint = round(len(df)*(train_valid_split/100))\n",
    "        cutpoint = 365\n",
    "\n",
    "        ##NOTE: in order to do train/valid split on time-based portion the input data must be sorted by date    \n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        df = df.sort_values(by='DATE', ascending=True)\n",
    "\n",
    "        y_valid = df[target_column][-cutpoint:].values.reshape(-1, 1)\n",
    "        X_valid = df[feature_columns][-cutpoint:].values\n",
    "        y_train = df[target_column][:-cutpoint].values.reshape(-1, 1)\n",
    "        X_train = df[feature_columns][:-cutpoint].values\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            max_epochs=max_epochs,\n",
    "            patience=100,\n",
    "            batch_size=1024, \n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False)\n",
    "\n",
    "        df['PRED'] = model.predict(tensor(df[feature_columns].round(2).values))\n",
    "        df['DATE'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "        df = pd.concat([df, pd.DataFrame(model.explain(df[feature_columns].values)[0], \n",
    "                               columns = feature_columns).add_prefix('EXPL_').round(2)], axis=1)\n",
    "    \n",
    "    return [df.values.tolist(), df.columns.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citibike_ml.mlops_pipeline import deploy_pred_train_udf\n",
    "\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS ' + model_stage_name).collect()\n",
    "\n",
    "model_udf_name = deploy_pred_train_udf(session=session, model_stage_name=model_stage_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The following code will not currently work due to a bug in the Snowpark backend.  This will be fixed in the 6.3.0 code push.  For now we use a limit function which essentially bypasses the row-wise parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = session.sql('USE WAREHOUSE LG_WH']).collect()\n",
    "\n",
    "max_epochs=10\n",
    "\n",
    "output_list = feature_df_stuffed.limit(1)\\\n",
    "                                .select('STATION_ID', F.call_udf(model_udf_name, \n",
    "                                                                 'INPUT_DATA', \n",
    "                                                                 'INPUT_COLUMN_LIST', \n",
    "                                                                 'TARGET_COLUMN', \n",
    "                                                                 F.lit(max_epochs)).alias('OUTPUT_DATA')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ast.literal_eval(output_list[0]['OUTPUT_DATA'])[0], \n",
    "                  columns = ast.literal_eval(output_list[0]['OUTPUT_DATA'])[1])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is essentially no changes to the actual training an prediction code except that we need to un-stuff the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile citibike_ml/parallel_udf.py\n",
    "\n",
    "def generate_feature_table(session, \n",
    "                           clone_table_name, \n",
    "                           feature_table_name, \n",
    "                           holiday_table_name, \n",
    "                           precip_table_name) -> list:\n",
    "    \n",
    "    from snowflake.snowpark import functions as F\n",
    "    import snowflake.snowpark as snp\n",
    "    \n",
    "    clone_df = session.table(clone_table_name)\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    precip_df = session.table(precip_table_name)\n",
    "\n",
    "    window = snp.Window.partitionBy(F.col('STATION_ID')).orderBy(F.col('DATE').asc())\n",
    "    sid_window = snp.Window.partitionBy(F.col('STATION_ID'))\n",
    "\n",
    "\n",
    "    feature_df = clone_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                                 F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                         .groupBy(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                            .count()\\\n",
    "                         .withColumn('DAY_COUNT', F.count(F.col('DATE')).over(sid_window))\\\n",
    "                            .filter(F.col('DAY_COUNT') >= 365*2)\\\n",
    "                         .withColumn('LAG_1', F.lag(F.col('COUNT'), offset=1, default_value=None).over(window))\\\n",
    "                         .withColumn('LAG_7', F.lag(F.col('COUNT'), offset=7, default_value=None).over(window))\\\n",
    "                            .na.drop()\\\n",
    "                         .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                         .join(precip_df, 'DATE', 'inner')\\\n",
    "                         .withColumn('DAY_COUNT', F.count(F.col('DATE')).over(sid_window))\\\n",
    "                            .filter(F.col('DAY_COUNT') >= 365*2)\\\n",
    "                         .drop('DAY_COUNT')\n",
    "    \n",
    "    feature_column_list = feature_df.columns\n",
    "    feature_column_list.remove('\\\"STATION_ID\\\"')\n",
    "    feature_column_list = [f.replace('\\\"', \"\") for f in feature_column_list]\n",
    "    feature_column_array = F.array_construct(*[F.lit(x) for x in feature_column_list])\n",
    "\n",
    "    feature_df_stuffed = feature_df.groupBy(F.col('STATION_ID'))\\\n",
    "                                   .agg(F.array_agg(F.array_construct(*feature_column_list)).alias('INPUT_DATA'))\\\n",
    "                                   .withColumn('INPUT_COLUMN_LIST', feature_column_array)\\\n",
    "                                   .withColumn('TARGET_COLUMN', F.lit('COUNT'))\n",
    "    \n",
    "    feature_df_stuffed.limit(50).write.mode('overwrite').saveAsTable(feature_table_name)        \n",
    "\n",
    "    return feature_table_name\n",
    "\n",
    "def train_predict_feature_table(session, station_train_pred_udf_name, feature_table_name, pred_table_name) -> str:\n",
    "    from snowflake.snowpark import functions as F\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    \n",
    "    max_epochs=10\n",
    "\n",
    "    output_list = session.table(feature_table_name)\\\n",
    "                         .select('STATION_ID', F.call_udf(station_train_pred_udf_name, \n",
    "                                                          'INPUT_DATA', \n",
    "                                                          'INPUT_COLUMN_LIST', \n",
    "                                                          'TARGET_COLUMN', \n",
    "                                                          F.lit(max_epochs)).alias('OUTPUT_DATA')).collect()\n",
    "    df = pd.DataFrame()\n",
    "    for row in range(len(output_list)):\n",
    "        tempdf = pd.DataFrame(data = ast.literal_eval(output_list[row]['OUTPUT_DATA'])[0], \n",
    "                                    columns=ast.literal_eval(output_list[row]['OUTPUT_DATA'])[1]\n",
    "                                    )\n",
    "        tempdf['STATION_ID'] = str(output_list[row]['STATION_ID'])\n",
    "        df = pd.concat([df, tempdf], axis=0)\n",
    "        \n",
    "    session.createDataFrame(df).write.mode('overwrite').saveAsTable(pred_table_name)\n",
    "    \n",
    "    return pred_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from citibike_ml.parallel_udf import generate_feature_table, train_predict_feature_table\n",
    "\n",
    "feature_table_name = generate_feature_table(session=session, \n",
    "                                            clone_table_name=trips_table_name, \n",
    "                                            feature_table_name='TRIPS_FEATURES_TEST', \n",
    "                                            holiday_table_name=holiday_table_name,\n",
    "                                            precip_table_name=precip_table_name\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(feature_table_name).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = session.sql('USE WAREHOUSE X2L_WH']).collect()\n",
    "\n",
    "pred_table_name = train_predict_feature_table(session=session, \n",
    "                                              station_train_pred_udf_name=model_udf_name, \n",
    "                                              feature_table_name=feature_table_name, \n",
    "                                              pred_table_name='PRED_TEST'\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(pred_table_name).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
