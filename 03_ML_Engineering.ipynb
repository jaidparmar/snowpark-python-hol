{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Engineering Development\n",
    "In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API to build and develope code for the **ML Ops pipeline**.  We will take the functions and model training/inference definition from the data scientist and put it into production using the Snowpark server-side runtime and Snowpark Python user-defined functions for ML model training and inference.\n",
    "\n",
    "The ML Engineer will start by exploring the deoployment options and testing the deployed model before building a pipeline.\n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The ML engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the ML engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.  Feature engineering, train, predict functions from data scientist.  \n",
    "Output: Prediction models available to business users in SQL. Evaluation reports for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect('./include/state.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Create Feature Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_table_name = state_dict['trips_table_name']\n",
    "holiday_table_name = state_dict['holiday_table_name']\n",
    "weather_table_name = state_dict['weather_table_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will materialize the holiday and weather datasets as tables instead of calculating each time in the inference and training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F \n",
    "from dags.feature_engineering import generate_holiday_df, generate_weather_df\n",
    "\n",
    "holiday_df = generate_holiday_df(session, holiday_table_name) \n",
    "holiday_df.write.mode('overwrite').saveAsTable(holiday_table_name)\n",
    "\n",
    "weather_df = generate_weather_df(session, weather_table_name) \n",
    "weather_df.write.mode('overwrite').saveAsTable(weather_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to forecast features for weather (TEMP) and also holidays.  For weather we could use forecast from the Snowflake Marketplace providers like Weather Source.  Since we went back in time for this hands-on-lab the weather \"forecast\" is actually in the historical weather tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forecast_df(session, holiday_table_name:str, weather_table_name:str, start_date:str, steps:int):\n",
    "    from dags.feature_engineering import generate_holiday_df, generate_weather_df\n",
    "    from datetime import timedelta, datetime\n",
    "    #import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F \n",
    "    \n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = start_date+timedelta(days=steps)\n",
    "\n",
    "    #check if it tables already materialized, otherwise generate DF\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns()\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "    weather_df = session.table(weather_table_name)[['DATE','TEMP']]\n",
    "    try: \n",
    "        _ = weather_df.columns()\n",
    "    except:\n",
    "        weather_df = generate_weather_df(session, weather_table_name)[['DATE','TEMP']]\n",
    "        \n",
    "    forecast_df = holiday_df.join(weather_df, 'DATE', join_type='right')\\\n",
    "                            .na.fill({'HOLIDAY':0})\\\n",
    "                            .filter((F.col('DATE') >= start_date) &\\\n",
    "                                    (F.col('DATE') <= end_date))\\\n",
    "                            .sort('DATE', ascending=True)\n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = create_forecast_df(session=session, \n",
    "                                 holiday_table_name=state_dict['holiday_table_name'], \n",
    "                                 weather_table_name=state_dict['weather_table_name'], \n",
    "                                 start_date='2020-03-01', \n",
    "                                 steps=30)\n",
    "forecast_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create UDF for Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a time series prediction we will retrain a model each time we do inference.  We don't need to save the model artefacts but we will save the predictions in an predictions table.  \n",
    "  \n",
    "Here we can use Snowpark User Defined Functions for training as well as inference without having to pull data out of Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/station_train_predict.py\n",
    "\n",
    "def station_train_predict_func(historical_data:list, \n",
    "                               historical_column_names:list, \n",
    "                               target_column:str,\n",
    "                               cutpoint: int, \n",
    "                               max_epochs: int, \n",
    "                               forecast_data:list,\n",
    "                               forecast_column_names:list,\n",
    "                               lag_values:list) -> str:\n",
    "    \n",
    "    from torch import tensor\n",
    "    import pandas as pd\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "    \n",
    "    feature_columns = historical_column_names.copy()\n",
    "    feature_columns.remove('DATE')\n",
    "    feature_columns.remove('STATION_ID')\n",
    "    feature_columns.remove(target_column)\n",
    "    \n",
    "    df = pd.DataFrame(historical_data, columns = historical_column_names)\n",
    "    #df['DATE']=pd.to_datetime(df['DATE']).dt.date\n",
    "    \n",
    "    y_valid = df[target_column][-cutpoint:].values.reshape(-1, 1)\n",
    "    X_valid = df[feature_columns][-cutpoint:].values\n",
    "    y_train = df[target_column][:-cutpoint].values.reshape(-1, 1)\n",
    "    X_train = df[feature_columns][:-cutpoint].values\n",
    "    \n",
    "    batch_df = pd.DataFrame(range(2,65,2), columns=['batch_size'])\n",
    "    batch_df['batch_remainder'] = len(X_train)%batch_df['batch_size']\n",
    "    optimal_batch_size=int(batch_df['batch_size'].where(batch_df['batch_remainder']==batch_df['batch_remainder'].min()).max())\n",
    "    \n",
    "    model = TabNetRegressor()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=100,\n",
    "        batch_size=optimal_batch_size, \n",
    "        virtual_batch_size=optimal_batch_size/2,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "    \n",
    "    df['PRED'] = model.predict(tensor(df[feature_columns].values))\n",
    "\n",
    "    #Now make the multi-step forecast\n",
    "    if len(lag_values) > 0:\n",
    "        forecast_df = pd.DataFrame(forecast_data, columns = forecast_column_names)\n",
    "        \n",
    "        for step in range(len(forecast_df)):\n",
    "            station_id = df['STATION_ID'][-1:].values[0]\n",
    "            future_date = (pd.to_datetime(df.iloc[-1]['DATE'])+timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            lags=[df.shift(lag-1).iloc[-1]['COUNT'] for lag in lag_values]\n",
    "            forecast=forecast_df.loc[forecast_df['DATE']==future_date]\n",
    "            forecast=forecast.drop(labels='DATE', axis=1).values.tolist()[0]\n",
    "            features=[*lags, *forecast]\n",
    "            pred=round(model.predict(np.array([features]))[0][0])\n",
    "            row=[future_date, station_id, pred, *features, pred]\n",
    "            df.loc[len(df)]=row\n",
    "    \n",
    "    explain_df = pd.DataFrame(model.explain(df[feature_columns].astype(float).values)[0], \n",
    "                         columns = feature_columns).add_prefix('EXPL_').round(2)\n",
    "    df = pd.concat([df.set_index('DATE').reset_index(), explain_df], axis=1)\n",
    "   \n",
    "    return [df.values.tolist(), df.columns.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Snowpark server-side Anaconda runtime has a large [list of Python modules included](https://docs.snowflake.com/en/LIMITEDACCESS/udf-python-packages.html#list-of-the-third-party-packages-from-anaconda) for our UDF.  However, the data scientist built this code based on pytorch-tabnet which is not currently in the Snowpark distribution.\n",
    "  \n",
    "  We can simply add [pytorch_tabnet](https://github.com/dreamquark-ai/tabnet), as well as our own team's python code, as import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.station_train_predict import station_train_predict_func\n",
    "import os \n",
    "\n",
    "#We can add dependencies from locally installed directories\n",
    "#source_dir = os.environ['CONDA_PREFIX']+'/lib/python3.8/site-packages/'\n",
    "\n",
    "model_stage_name = 'model_stage'\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS '+str(model_stage_name)).collect()\n",
    "\n",
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "session.add_packages([\"pandas==1.3.5\", \"pytorch\", \"scipy\", \"scikit-learn\"])\n",
    "session.add_import('./include/pytorch_tabnet.zip')\n",
    "session.add_import('dags')\n",
    "\n",
    "station_train_predict_udf = session.udf.register(station_train_predict_func, \n",
    "                                              name=\"station_train_predict_udf\",\n",
    "                                              is_permanent=True,\n",
    "                                              stage_location='@'+str(model_stage_name), \n",
    "                                              replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the training/inference pipeline and prediction output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an array aggregation to feed the training data to our UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dags.feature_engineering import generate_features\n",
    "\n",
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F \n",
    "\n",
    "station_id = '519'\n",
    "target_column = 'COUNT'\n",
    "lag_values=[1,7,365]\n",
    "forecast_days=30\n",
    "start_date='2020-03-01'\n",
    "\n",
    "snowdf = session.table(state_dict['trips_table_name']).filter(F.col('START_STATION_ID') == station_id)\n",
    "\n",
    "historical_df = generate_features(session=session, \n",
    "                                 input_df=snowdf, \n",
    "                                 holiday_table_name=state_dict['holiday_table_name'], \n",
    "                                 weather_table_name=state_dict['weather_table_name'])\n",
    "\n",
    "forecast_df = create_forecast_df(session=session, \n",
    "                                 holiday_table_name=state_dict['holiday_table_name'], \n",
    "                                 weather_table_name=state_dict['weather_table_name'], \n",
    "                                 start_date=start_date, \n",
    "                                 steps=forecast_days)\n",
    "\n",
    "#Until UDTF we need to do array stuffing on these two dataframes\n",
    "historical_column_names = F.array_construct(*[F.lit(x) for x in historical_df.columns])\n",
    "forecast_column_names = F.array_construct(*[F.lit(x) for x in forecast_df.columns])\n",
    "lag_values_array = F.array_construct(*[F.lit(x) for x in lag_values])\n",
    "\n",
    "historical_df = historical_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('HISTORICAL_DATA'))\n",
    "forecast_df = forecast_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('FORECAST_DATA'))\n",
    "\n",
    "cutpoint=365\n",
    "max_epochs = 100\n",
    "\n",
    "output_df = historical_df.join(forecast_df)\\\n",
    "                         .select(F.call_udf('station_train_predict_udf', \n",
    "                                            F.col('HISTORICAL_DATA'),\n",
    "                                            F.lit(historical_column_names), \n",
    "                                            F.lit(target_column),\n",
    "                                            F.lit(cutpoint), \n",
    "                                            F.lit(max_epochs),\n",
    "                                            F.col('FORECAST_DATA'),\n",
    "                                            F.lit(forecast_column_names),\n",
    "                                            F.lit(lag_values_array))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.DataFrame(data = ast.literal_eval(output_df[0][0])[0], \n",
    "                  columns = ast.literal_eval(output_df[0][0])[1])\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE']).dt.date\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, x_lab:str, y_true_lab:str, y_pred_lab:str):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    df = pd.melt(df, id_vars=[x_lab], value_vars=[y_true_lab, y_pred_lab])\n",
    "    ax = sns.lineplot(x=x_lab, y='value', hue='variable', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "   \n",
    "plot(df, 'DATE', 'COUNT', 'PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will end by consolidating the functions we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile citibike_ml/mlops_pipeline.py\n",
    "\n",
    "# def materialize_holiday_table(session, start_date, end_date, holiday_table_name) -> str:\n",
    "#     from citibike_ml.feature_engineering import generate_holiday_df\n",
    "#     from datetime import datetime\n",
    "\n",
    "#     holiday_df = generate_holiday_df(session=session, start_date=start_date, end_date=datetime.now())\n",
    "#     holiday_df.write.mode('overwrite').saveAsTable(holiday_table_name)\n",
    "    \n",
    "#     return holiday_table_name\n",
    "\n",
    "# def materialize_weather_table(session, start_date, end_date, weather_table_name) -> str:\n",
    "#     from citibike_ml.feature_engineering import generate_weather_df\n",
    "#     from datetime import datetime\n",
    "\n",
    "#     weather_df = generate_weather_df(session=session, start_date=start_date, end_date=datetime.now())\n",
    "#     weather_df.write.mode('overwrite').saveAsTable(weather_table_name)\n",
    "    \n",
    "#     return weather_table_name\n",
    "\n",
    "# def deploy_pred_train_udf(session, function_name, model_stage_name) -> str:\n",
    "#     from citibike_ml.station_train_predict import station_train_predict_func\n",
    "\n",
    "#     dep = 'pytorch_tabnet.zip'\n",
    "#     source_dir = './include/'\n",
    "\n",
    "#     session.clearImports()\n",
    "#     session.addImport(source_dir+dep)\n",
    "#     session.addImport('citibike_ml')\n",
    "\n",
    "#     station_train_predict_udf = session.udf.register(station_train_predict_func, \n",
    "#                                                   name=\"station_train_predict_udf\",\n",
    "#                                                   is_permanent=True,\n",
    "#                                                   stage_location='@'+str(model_stage_name), \n",
    "#                                                   replace=True)\n",
    "#     return station_train_predict_udf.name\n",
    "\n",
    "\n",
    "# def generate_feature_views(session, \n",
    "#                            clone_table_name, \n",
    "#                            feature_view_name, \n",
    "#                            holiday_table_name, \n",
    "#                            weather_table_name, \n",
    "#                            target_column, \n",
    "#                            top_n) -> list:\n",
    "#     from citibike_ml.feature_engineering import generate_features\n",
    "#     from snowflake.snowpark import functions as F\n",
    "\n",
    "#     feature_view_names = list()\n",
    "    \n",
    "#     top_n_station_ids = session.table(clone_table_name).filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "#                                                        .groupBy('START_STATION_ID') \\\n",
    "#                                                        .count() \\\n",
    "#                                                        .sort('COUNT', ascending=False) \\\n",
    "#                                                        .limit(top_n) \\\n",
    "#                                                        .collect()\n",
    "#     top_n_station_ids = [stations['START_STATION_ID'] for stations in top_n_station_ids]\n",
    "\n",
    "#     for station in top_n_station_ids:\n",
    "#         feature_df = generate_features(session=session, \n",
    "#                                        input_df=session.table(clone_table_name)\\\n",
    "#                                                        .filter(F.col('START_STATION_ID') == station), \n",
    "#                                        holiday_table_name=holiday_table_name, \n",
    "#                                        weather_table_name=weather_table_name)\n",
    "\n",
    "#         input_columns_str = str(' ').join(feature_df.columns).replace('\\\"', \"\")\n",
    "\n",
    "#         feature_df = feature_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('input_data'), \n",
    "#                                        F.lit(station).alias('station_id'),\n",
    "#                                        F.lit(input_columns_str).alias('input_column_names'),\n",
    "#                                        F.lit(target_column).alias('target_column'))  \n",
    "\n",
    "#         station_feature_view_name = feature_view_name.replace('<station_id>', station)\n",
    "#         feature_df.createOrReplaceView(station_feature_view_name)\n",
    "#         feature_view_names.append(station_feature_view_name)\n",
    "\n",
    "#     return feature_view_names\n",
    "\n",
    "\n",
    "# def train_predict_feature_views(session, station_train_pred_udf_name, feature_view_names, pred_table_name) -> str:\n",
    "#     from snowflake.snowpark import functions as F\n",
    "#     import pandas as pd\n",
    "#     import ast\n",
    "    \n",
    "#     cutpoint=365\n",
    "#     max_epochs=1000\n",
    "    \n",
    "#     for view in feature_view_names:\n",
    "#         feature_df = session.table(view)\n",
    "#         output_df = feature_df.select(F.call_udf(station_train_pred_udf_name, \n",
    "#                                                  'INPUT_DATA', \n",
    "#                                                  'INPUT_COLUMN_NAMES', \n",
    "#                                                  'TARGET_COLUMN', \n",
    "#                                                  F.lit(cutpoint), \n",
    "#                                                  F.lit(max_epochs))).collect()\n",
    "\n",
    "#         df = pd.DataFrame(data = ast.literal_eval(output_df[0][0])[0], \n",
    "#                       columns = ast.literal_eval(output_df[0][0])[1])\n",
    "\n",
    "#         df['DATE'] = pd.to_datetime(df['DATE']).dt.date\n",
    "#         df['STATION_ID'] = feature_df.select('STATION_ID').collect()[0][0]\n",
    "\n",
    "#         output_df = session.createDataFrame(df).write.saveAsTable(pred_table_name)\n",
    "    \n",
    "#     return pred_table_name\n",
    "\n",
    "# def create_forecast_df(session, holiday_table_name:str, weather_table_name:str, start_date:str, steps:int):\n",
    "#     from dags.feature_engineering import generate_holiday_df, generate_weather_df\n",
    "#     from datetime import timedelta, datetime\n",
    "#     #import snowflake.snowpark as snp\n",
    "#     from snowflake.snowpark import functions as F \n",
    "    \n",
    "#     start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "#     end_date = start_date+timedelta(days=steps)\n",
    "\n",
    "#     #check if it tables already materialized, otherwise generate DF\n",
    "#     holiday_df = session.table(holiday_table_name)\n",
    "#     try: \n",
    "#         _ = holiday_df.columns()\n",
    "#     except:\n",
    "#         holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "#     weather_df = session.table(weather_table_name)[['DATE','TEMP']]\n",
    "#     try: \n",
    "#         _ = weather_df.columns()\n",
    "#     except:\n",
    "#         weather_df = generate_weather_df(session, weather_table_name)[['DATE','TEMP']]\n",
    "        \n",
    "#     forecast_df = holiday_df.join(weather_df, 'DATE', join_type='right')\\\n",
    "#                             .na.fill({'HOLIDAY':0})\\\n",
    "#                             .filter((F.col('DATE') >= start_date) &\\\n",
    "#                                     (F.col('DATE') <= end_date))\\\n",
    "#                             .sort('DATE', ascending=True)\n",
    "#     return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
