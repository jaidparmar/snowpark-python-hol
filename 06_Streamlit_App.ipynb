{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e5c1e4",
   "metadata": {},
   "source": [
    "# Citibike ML\n",
    "In this example we use the [Citibike dataset](https://ride.citibikenyc.com/system-data). Citibike is a bicycle sharing system in New York City. Everyday users choose from 20,000 bicycles at 1300 stations around New York City.\n",
    "\n",
    "To ensure customer satisfaction Citibike needs to predict how many bicycles will be needed at each station. Maintenance teams from Citibike will check each station and repair or replace bicycles. Additionally, the team will relocate bicycles between stations based on predicted demand. The business needs to be able to run reports of how many bicycles will be needed at a given station on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21e38c",
   "metadata": {},
   "source": [
    "## Streamlit Application\n",
    "In this section of the demo, we will utilize Streamlit with Snowpark's Python client-side Dataframe API to create a visual front-end application for the Citibike operations team to consume the insights from the ML forecast.\n",
    "\n",
    "For this demo flow we will assume that the organization has the following **policies and processes** :   \n",
    "-**Dev Tools**: The ML engineer can develop in their tool of choice (ie. VS Code, IntelliJ, Pycharm, Eclipse, etc.).  Snowpark Python makes it possible to use any environment where they have a python kernel.  For the sake of a demo we will use Jupyter.  \n",
    "-**Data Governance**: To preserve customer privacy no data can be stored locally.  The ingest system may store data temporarily but it must be assumed that, in production, the ingest system will not preserve intermediate data products between runs. Snowpark Python allows the user to push-down all operations to Snowflake and bring the code to the data.   \n",
    "-**Automation**: Although the ML engineer can use any IDE or notebooks for development purposes the final product must be python code at the end of the work stream.  Well-documented, modularized code is necessary for good ML operations and to interface with the company's CI/CD and orchestration tools.  \n",
    "-**Compliance**: Any ML models must be traceable back to the original data set used for training.  The business needs to be able to easily remove specific user data from training datasets and retrain models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3bec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip -q install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66099ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app.py\n",
    "from snowflake.snowpark import functions as F\n",
    "from dags.snowpark_connection import snowpark_connect\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil.relativedelta import *\n",
    "import calendar\n",
    "import altair as alt\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logging.getLogger().setLevel(logging.WARN)\n",
    "\n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "\n",
    "\n",
    "def update_forecast_table(forecast_df, stations:list, start_date, end_date):\n",
    "#     explainer_columns = [col for col in forecast_df.schema.names if 'EXP' in col]\n",
    "    explainer_columns=['EXPL_LAG_1', 'EXPL_LAG_7','EXPL_LAG_365','EXPL_HOLIDAY','EXPL_TEMP']\n",
    "    explainer_columns_new=['DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR','US_HOLIDAY', 'TEMPERATURE']\n",
    "\n",
    "    cond = \"F.when\" + \".when\".join([\"(F.col('\" + c + \"') == F.col('EXPLAIN'), F.lit('\" + c + \"'))\" for c in explainer_columns])\n",
    "\n",
    "    df = forecast_df.filter((forecast_df['STATION_ID'].in_(stations)) &\n",
    "                       (F.col('DATE') >= start_date) & \n",
    "                       (F.col('DATE') <= end_date))\\\n",
    "                .select(['STATION_ID', \n",
    "                         F.to_char(F.col('DATE')).alias('DATE'), \n",
    "                         'PRED', \n",
    "                         'HOLIDAY',\n",
    "                         *explainer_columns])\\\n",
    "                .with_column('EXPLAIN', F.greatest(*explainer_columns))\\\n",
    "                .with_column('REASON', eval(cond))\\\n",
    "                .select(F.col('STATION_ID'), \n",
    "                        F.col('DATE'), \n",
    "                        F.col('PRED'), \n",
    "                        F.col('REASON'), \n",
    "                        F.col('EXPLAIN'), \n",
    "                        F.col('EXPL_LAG_1').alias('DAY'),\n",
    "                        F.col('EXPL_LAG_7').alias('DAY_OF_WEEK'),\n",
    "                        F.col('EXPL_LAG_365').alias('DAY_OF_YEAR'),\n",
    "                        F.col('EXPL_HOLIDAY').alias('US_HOLIDAY'),\n",
    "                        F.col('EXPL_TEMP').alias('TEMPERATURE'),\n",
    "                       )\\\n",
    "                .to_pandas()\n",
    "    \n",
    "    df['REASON'] = pd.Categorical(df['REASON'])\n",
    "    df['REASON_CODE']=df['REASON'].cat.codes\n",
    "        \n",
    "    rect = alt.Chart(df).mark_rect().encode(alt.X('DATE:N'), \n",
    "                                        alt.Y('STATION_ID:N'), \n",
    "                                        alt.Color('REASON'),\n",
    "                                        tooltip=explainer_columns_new)\n",
    "    text = rect.mark_text(baseline='middle').encode(text='PRED:Q', color=alt.value('white'))\n",
    "\n",
    "    l = alt.layer(\n",
    "        rect, text\n",
    "    )\n",
    "\n",
    "    st.write(\"### Forecast\")\n",
    "    st.altair_chart(l, use_container_width=True)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def update_eval_table(eval_df, stations:list):\n",
    "    df = eval_df.select('STATION_ID', F.to_char(F.col('RUN_DATE')).alias('RUN_DATE'), 'RMSE')\\\n",
    "                .filter(eval_df['STATION_ID'].in_(stations))\\\n",
    "                .to_pandas()\n",
    "\n",
    "    data = df.pivot(index=\"RUN_DATE\", columns=\"STATION_ID\", values=\"RMSE\")\n",
    "    data = data.reset_index().melt('RUN_DATE', var_name='STATION_ID', value_name='RMSE')\n",
    "\n",
    "    nearest = alt.selection(type='single', nearest=True, on='mouseover',\n",
    "                            fields=['RUN_DATE'], empty='none')\n",
    "\n",
    "    line = alt.Chart(data).mark_line(interpolate='basis').encode(\n",
    "        x='RUN_DATE:N',\n",
    "        y='RMSE:Q',\n",
    "        color='STATION_ID:N'\n",
    "    )\n",
    "\n",
    "    selectors = alt.Chart(data).mark_point().encode(\n",
    "        x='RUN_DATE:N',\n",
    "        opacity=alt.value(0)\n",
    "    ).add_selection(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    points = line.mark_point().encode(\n",
    "        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "    )\n",
    "\n",
    "    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "        text=alt.condition(nearest, 'RMSE:Q', alt.value(' '))\n",
    "    )\n",
    "\n",
    "    rules = alt.Chart(data).mark_rule(color='gray').encode(\n",
    "        x='RUN_DATE:N',\n",
    "    ).transform_filter(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    l = alt.layer(\n",
    "        line, selectors, points, rules, text\n",
    "    ).properties(\n",
    "        width=600, height=300\n",
    "    )\n",
    "    st.write(\"### Model Monitor (RMSE)\")\n",
    "    st.altair_chart(l, use_container_width=True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "forecast_df = session.table('FLAT_FORECAST')\n",
    "eval_df = session.table('FLAT_EVAL')\n",
    "\n",
    "min_date=session.table('FLAT_FORECAST').select(F.min('DATE')).collect()[0][0]\n",
    "max_date=session.table('FLAT_FORECAST').select(F.max('DATE')).collect()[0][0]\n",
    "\n",
    "start_date = st.date_input('Start Date', value=min_date, min_value=min_date, max_value=max_date)\n",
    "show_days = st.number_input('Number of days to show', value=7, min_value=1, max_value=30)\n",
    "end_date = start_date+timedelta(days=show_days)\n",
    "\n",
    "stations_df=session.table('FLAT_FORECAST').select(F.col('STATION_ID')).distinct().to_pandas()\n",
    "\n",
    "sample_stations = [\"519\", \"497\", \"435\", \"402\", \"426\", \"285\", \"293\"]\n",
    "\n",
    "stations = st.multiselect('Choose stations', stations_df['STATION_ID'], sample_stations)\n",
    "if not stations:\n",
    "    stations = stations_df['STATION_ID']\n",
    "\n",
    "update_forecast_table(forecast_df, stations, start_date, end_date)\n",
    "\n",
    "update_eval_table(eval_df, stations)\n",
    "\n",
    "last_trip_date = session.table('TRIPS').select(F.to_date(F.max('STARTTIME'))).collect()[0][0]\n",
    "\n",
    "next_ingest = last_trip_date+relativedelta(months=+1)\n",
    "next_ingest = next_ingest.replace(day=1)       \n",
    "\n",
    "if next_ingest <= datetime.strptime(\"2016-12-01\", \"%Y-%m-%d\").date():\n",
    "    download_file_name=next_ingest.strftime('%Y%m')+'-citibike-tripdata.zip'\n",
    "else:\n",
    "    download_file_name=next_ingest.strftime('%Y%m')+'-citibike-tripdata.csv.zip'\n",
    "    \n",
    "run_date = next_ingest+relativedelta(months=+1)\n",
    "run_date = run_date.strftime('%Y_%m_%d')\n",
    "\n",
    "st.write('Data provided as of '+str(last_trip_date))\n",
    "\n",
    "st.write('Next ingest for '+str(next_ingest))\n",
    "\n",
    "def trigger_ingest(download_file_name, run_date):    \n",
    "    dag_url='http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns'\n",
    "    json_payload = {\"conf\": {\"files_to_download\": [download_file_name], \"run_date\": run_date}}\n",
    "    \n",
    "    response = requests.post(dag_url, \n",
    "                            json=json_payload,\n",
    "                            auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "    run_id = json.loads(response.text)['dag_run_id']\n",
    "    #run_id = 'manual__2022-04-07T15:02:29.166108+00:00'\n",
    "\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "    st.snow()\n",
    "\n",
    "    with st.spinner('Ingesting file: '+download_file_name):\n",
    "        while state != 'success':\n",
    "            time.sleep(10)\n",
    "            state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "    st.success('Ingested file: '+download_file_name+' State: '+str(state))\n",
    "\n",
    "st.button('Run Ingest Taskflow', on_click=trigger_ingest, args=(download_file_name, run_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run --theme.base dark streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15d51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
